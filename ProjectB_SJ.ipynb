{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JadhavShrutiS/CSCE580_F25/blob/main/ProjectB_SJ.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CSCE 580-Project B\n",
        "by Shruti Jadhav"
      ],
      "metadata": {
        "id": "8qerbDclf7ZG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### First all required packages must be installed and GPU is set to ensure training happens faster (compared to training on CPU)"
      ],
      "metadata": {
        "id": "bSwAsQYXgMf1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages (Colab / fresh env)\n",
        "!pip install -q -U transformers accelerate datasets evaluate scikit-learn matplotlib seaborn pyarrow\n",
        "!pip install -q -U datasets"
      ],
      "metadata": {
        "id": "dj86zvCaJ0Eq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GPU check (works in Colab after Runtime -> Change runtime type -> GPU)\n",
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "    print('Using GPU:', torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print('GPU not available. The notebook will still run on CPU but training is much slower.')\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n"
      ],
      "metadata": {
        "id": "FAqh-iKcFmBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "fh50hF0eGr0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### First we import the csv data. For easy access to data by all, it is hosted and extracted from a Github repo."
      ],
      "metadata": {
        "id": "rtMONefMbcqq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/JadhavShrutiS/CSCE580/refs/heads/main/Project-B/IMDB%20Dataset.csv\"\n",
        "df = pd.read_csv(url)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "_WCxd7RQGwmd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Then we use a Stratified Split to split the csv data into train, test and validation data segements. Here 80% data is used to train, 10% is used to validate, and 10% is used for final testing."
      ],
      "metadata": {
        "id": "mWTvcarTbury"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
        "\n",
        "ds = Dataset.from_pandas(df)\n",
        "\n",
        "reviews = np.array(ds[\"review\"])\n",
        "sentiments = np.array(ds[\"sentiment\"])\n",
        "label_map = {\"positive\": 1, \"negative\": 0}\n",
        "labels = np.array([label_map[x] for x in sentiments])\n",
        "\n",
        "# 80% train, 20% holdout\n",
        "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=50)\n",
        "train_idx, hold_idx = next(sss.split(reviews, labels))\n",
        "\n",
        "hold_labels = labels[hold_idx]\n",
        "hold_reviews = reviews[hold_idx]\n",
        "\n",
        "# 50/50 val and test\n",
        "val_idx, test_idx = train_test_split(\n",
        "    np.arange(len(hold_reviews)),\n",
        "    test_size=0.5,\n",
        "    stratify=hold_labels,\n",
        "    random_state=50\n",
        ")\n",
        "\n",
        "train_ds = Dataset.from_dict({\n",
        "    \"review\": reviews[train_idx],\n",
        "    \"labels\": labels[train_idx]\n",
        "})\n",
        "\n",
        "val_ds = Dataset.from_dict({\n",
        "    \"review\": hold_reviews[val_idx],\n",
        "    \"labels\": hold_labels[val_idx]\n",
        "})\n",
        "\n",
        "test_ds = Dataset.from_dict({\n",
        "    \"review\": hold_reviews[test_idx],\n",
        "    \"labels\": hold_labels[test_idx]\n",
        "})\n"
      ],
      "metadata": {
        "id": "-I-Ovfj1ECQt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Following splitting the data, we processing the data by first tokenizing. We truncate the reviews to a maximum of 256 tokenized text to allow for better handling with the model."
      ],
      "metadata": {
        "id": "tEb0MONLcHKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "MODEL_NAME = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "def tokenize_fn(batch):\n",
        "    return tokenizer(\n",
        "        batch[\"review\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=256\n",
        "    )\n",
        "\n",
        "train_tok = train_ds.map(tokenize_fn, batched=True)\n",
        "val_tok   = val_ds.map(tokenize_fn, batched=True)\n",
        "test_tok  = test_ds.map(tokenize_fn, batched=True)\n"
      ],
      "metadata": {
        "id": "D566ov57EPR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Then we finish the data processing step by padding the data. This ensures all batches of data being processed have the same length. After the padding is done, the training starts.\n",
        "\n",
        "#### Classification is configured using binary outputs (num_labels =2), and evaluation metrics are defined using the compute_metrics function."
      ],
      "metadata": {
        "id": "qJdu1YhQc5dR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "from transformers import DataCollatorWithPadding\n",
        "import evaluate\n",
        "import numpy as np\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    num_labels=2\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer)"
      ],
      "metadata": {
        "id": "f7M-AgSdGO8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "import evaluate\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    acc = evaluate.load(\"accuracy\")\n",
        "    f1 = evaluate.load(\"f1\")\n",
        "    prec = evaluate.load(\"precision\")\n",
        "    rec = evaluate.load(\"recall\")\n",
        "\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": acc.compute(predictions=preds, references=labels)[\"accuracy\"],\n",
        "        \"precision\": prec.compute(predictions=preds, references=labels)[\"precision\"],\n",
        "        \"recall\": rec.compute(predictions=preds, references=labels)[\"recall\"],\n",
        "        \"f1\": f1.compute(predictions=preds, references=labels)[\"f1\"]\n",
        "    }\n",
        "\n",
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "OUT_DIR = \"distilbert_model\"\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUT_DIR,\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    num_train_epochs=5,\n",
        "    eval_strategy=\"epoch\",\n",
        "    weight_decay=0.01,\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_steps=50,\n",
        "    load_best_model_at_end=True\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_tok,\n",
        "    eval_dataset=val_tok,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "metadata": {
        "id": "eSyszlVKRKCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### The training is now started and the training time is recorded."
      ],
      "metadata": {
        "id": "3PpmuQqvhOH3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "trainer.train()\n",
        "end_time = time.time()\n",
        "training_time = end_time - start_time\n",
        "print(f\"Training time (fine-tuned DistilBERT): {training_time:.2f} seconds\")"
      ],
      "metadata": {
        "id": "xGZNAJiKrBvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### After training, the final trained model is saved. This is intially downloaded and then uploaded to huggingface.\n",
        "\n",
        "#### This ensures that training is only done once. The trained information can be extracted from the saved files."
      ],
      "metadata": {
        "id": "1TOOMwRVhVaw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FIN_DIR = \"distilbert_final\"\n",
        "trainer.save_model(FIN_DIR)\n",
        "tokenizer.save_pretrained(FIN_DIR)"
      ],
      "metadata": {
        "id": "3uzZcS2PM3I2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# shutil.make_archive(\"distilbert_checkpoints_zip\", 'zip', OUT_DIR)\n",
        "shutil.make_archive(\"distilbert_final_zip\", 'zip', FIN_DIR)"
      ],
      "metadata": {
        "id": "BwLAsrXkAOfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "# files.download(\"distilbert_checkpoints_zip.zip\")\n",
        "files.download(\"distilbert_final_zip.zip\")"
      ],
      "metadata": {
        "id": "PnMB1GBcCvLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### The following set of code is used to extract the once trained data from fine-tuned DistilBERT. So that even after training once, the rest of the analysis can be run independently."
      ],
      "metadata": {
        "id": "8vJNOwAOfAT2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "\n",
        "model_path = \"ShrutiJadh/Distilbert_finetuned_SJ\"\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)"
      ],
      "metadata": {
        "id": "qteYmPbjcDFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### The fine-tuned DistilBERT model is now evaluated on the test set."
      ],
      "metadata": {
        "id": "MtrJjqa7ehxx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate(test_tok)"
      ],
      "metadata": {
        "id": "qpmIfE9hM4IT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### The metrics are extracted from the trained data for the fine-tuned DistilBERT case:"
      ],
      "metadata": {
        "id": "KCoMN3V_etHU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_eval = trainer.evaluate(train_tok.select(range(300)))\n",
        "train_accuracy_value = train_eval[\"eval_accuracy\"]\n"
      ],
      "metadata": {
        "id": "KvQjmStre5hk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_accuracy_list = [(i+1, train_accuracy_value) for i in range(training_args.num_train_epochs)]"
      ],
      "metadata": {
        "id": "BcVz_49vjQ2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### The graphs for accuracy and loss are then constructed using the extracted metrics."
      ],
      "metadata": {
        "id": "nC51tFo8e0uF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "logs = pd.DataFrame(trainer.state.log_history)\n",
        "\n",
        "# Keep only entries that have BOTH epoch and eval metrics\n",
        "eval_acc_df = logs[\n",
        "    logs[\"eval_accuracy\"].notna() &\n",
        "    logs[\"epoch\"].notna()\n",
        "][[\"epoch\", \"eval_accuracy\"]].drop_duplicates(subset=\"epoch\")\n",
        "\n",
        "train_loss_df = logs[\n",
        "    logs[\"loss\"].notna() &\n",
        "    logs[\"epoch\"].notna()\n",
        "][[\"epoch\", \"loss\"]].drop_duplicates(subset=\"epoch\")\n",
        "\n",
        "eval_loss_df = logs[\n",
        "    logs[\"eval_loss\"].notna() &\n",
        "    logs[\"epoch\"].notna()\n",
        "][[\"epoch\", \"eval_loss\"]].drop_duplicates(subset=\"epoch\")\n",
        "\n",
        "eval_acc_df, train_loss_df, eval_loss_df\n"
      ],
      "metadata": {
        "id": "h3WB6rBQI6h-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(eval_acc_df[\"epoch\"], eval_acc_df[\"eval_accuracy\"], label = \"validation Accuracy\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Validation Accuracy\")\n",
        "plt.title(\"Validation Accuracy per Epoch\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# Loss graph\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(train_loss_df[\"epoch\"], train_loss_df[\"loss\"], label=\"Train Loss\")\n",
        "plt.plot(eval_loss_df[\"epoch\"], eval_loss_df[\"eval_loss\"], label=\"Validation Loss\")\n",
        "plt.title(\"Training and Validation Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "4UzphynbJWsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### The confusion matrix for fine tuned distilBERT model is constructed as:"
      ],
      "metadata": {
        "id": "hAO37Qxxhz8x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "preds_output = trainer.predict(test_tok)\n",
        "preds = np.argmax(preds_output.predictions, axis=-1)\n",
        "\n",
        "cm = confusion_matrix(test_ds[\"labels\"], preds)\n",
        "print(cm)\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "cm = confusion_matrix(test_ds[\"labels\"], preds)\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.heatmap(\n",
        "    cm,\n",
        "    annot=True,\n",
        "    fmt='d',\n",
        "    cmap='Blues',\n",
        "    xticklabels=[\"Negative\", \"Positive\"],\n",
        "    yticklabels=[\"Negative\", \"Positive\"]\n",
        ")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.title(\"Confusion Matrix - Fine-Tuned DistilBERT\")\n",
        "plt.show()\n",
        "\n",
        "print(classification_report(test_ds[\"labels\"], preds, target_names=[\"negative\",\"positive\"]))"
      ],
      "metadata": {
        "id": "mfkCrQRRFS2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Now the pretrained DistilBERT model without fine-tuning, is used to classify the set"
      ],
      "metadata": {
        "id": "U-Hb1kSfe3P2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "import numpy as np\n",
        "\n",
        "pretrained_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    MODEL_NAME, num_labels=2\n",
        ")\n",
        "\n",
        "pretrained_trainer = Trainer(\n",
        "    model=pretrained_model,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "pre_preds_output = pretrained_trainer.predict(test_tok)\n",
        "pre_preds = np.argmax(pre_preds_output.predictions, axis=-1)\n",
        "\n",
        "nonft_accuracy = accuracy_score(test_ds[\"labels\"], pre_preds)\n",
        "nonft_precision, nonft_recall, nonft_f1, _ = precision_recall_fscore_support(\n",
        "    test_ds[\"labels\"], pre_preds, average=\"binary\"\n",
        ")\n",
        "\n",
        "print(\"\\n===== NON-FINE-TUNED DISTILBERT METRICS =====\")\n",
        "print(\"Accuracy:\", nonft_accuracy)\n",
        "print(\"Precision:\", nonft_precision)\n",
        "print(\"Recall:\", nonft_recall)\n",
        "print(\"F1 Score:\", nonft_f1)\n",
        "\n",
        "print(confusion_matrix(test_ds[\"labels\"], pre_preds))\n",
        "\n",
        "cm_pre = confusion_matrix(test_ds[\"labels\"], pre_preds)\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.heatmap(\n",
        "    cm_pre,\n",
        "    annot=True,\n",
        "    fmt='d',\n",
        "    cmap='Purples',\n",
        "    xticklabels=[\"Negative\", \"Positive\"],\n",
        "    yticklabels=[\"Negative\", \"Positive\"]\n",
        ")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.title(\"Confusion Matrix - Pretrained DistilBERT\")\n",
        "plt.show()\n",
        "print(classification_report(test_ds[\"labels\"], pre_preds, target_names=[\"negative\",\"positive\"]))"
      ],
      "metadata": {
        "id": "FKlBesZ_FWZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Following is the Classical Machine Learning model that uses Support Vector Machine using TF-IDF features"
      ],
      "metadata": {
        "id": "aCeVt86HeN_p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_recall_fscore_support\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.pipeline import Pipeline\n",
        "import time\n",
        "\n",
        "clf = Pipeline([\n",
        "    (\"tfidf\", TfidfVectorizer(max_features=20000)),\n",
        "    (\"svm\", LinearSVC())\n",
        "])\n",
        "start = time.time()\n",
        "clf.fit(train_ds[\"review\"], train_ds[\"labels\"])\n",
        "end = time.time()\n",
        "\n",
        "print(f\"Training time: {end - start:.2f} seconds\")\n",
        "svm_preds = clf.predict(test_ds[\"review\"])\n",
        "\n",
        "svm_accuracy = accuracy_score(test_ds[\"labels\"], svm_preds)\n",
        "svm_precision, svm_recall, svm_f1, _ = precision_recall_fscore_support(\n",
        "    test_ds[\"labels\"], svm_preds, average=\"binary\"\n",
        ")\n",
        "\n",
        "print(\"\\n===== SVM METRICS =====\")\n",
        "print(\"Accuracy:\", svm_accuracy)\n",
        "print(\"Precision:\", svm_precision)\n",
        "print(\"Recall:\", svm_recall)\n",
        "print(\"F1 Score:\", svm_f1)\n",
        "\n",
        "# print(confusion_matrix(test_ds[\"labels\"], svm_preds))\n",
        "cm_svm = confusion_matrix(test_ds[\"labels\"], svm_preds)\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.heatmap(\n",
        "    cm_svm,\n",
        "    annot=True,\n",
        "    fmt='d',\n",
        "    cmap='Greens',\n",
        "    xticklabels=[\"Negative\", \"Positive\"],\n",
        "    yticklabels=[\"Negative\", \"Positive\"]\n",
        ")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.title(\"Confusion Matrix - SVM Classifier\")\n",
        "plt.show()\n",
        "\n",
        "print(classification_report(test_ds[\"labels\"], svm_preds, target_names=[\"negative\",\"positive\"]))"
      ],
      "metadata": {
        "id": "7Kii4OlRFZ3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Lastly, functions are written to predict any user provided review."
      ],
      "metadata": {
        "id": "SChKiEsxdk-K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "def predict_finetuned(text):\n",
        "    import time\n",
        "    start = time.time()\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=256)\n",
        "    # move inputs to the SAME device\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    pred = outputs.logits.argmax(dim=1).item()\n",
        "    end = time.time()\n",
        "    print(f\"Inference time: {end -start:.4f} seconds\")\n",
        "\n",
        "    return \"positive\" if pred == 1 else \"negative\""
      ],
      "metadata": {
        "id": "Cx2MCGZsNlOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained_model.to(device)\n",
        "pretrained_model.eval()\n",
        "\n",
        "def predict_pretrained(text):\n",
        "    import time\n",
        "    start = time.time()\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=256)\n",
        "    # move inputs to the SAME device\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    pred = outputs.logits.argmax(dim=1).item()\n",
        "    end = time.time()\n",
        "    print(f\"Inference time: {end -start:.4f} seconds\")\n",
        "    return \"positive\" if pred == 1 else \"negative\""
      ],
      "metadata": {
        "id": "OqK2Z7_TFf2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_svm(text):\n",
        "    import time\n",
        "    start = time.time()\n",
        "    pred = clf.predict([text])[0]\n",
        "    end = time.time()\n",
        "    print(f\"Inference time: {end -start:.4f} seconds\")\n",
        "    return \"positive\" if pred == 1 else \"negative\""
      ],
      "metadata": {
        "id": "cTxvT1a7Flck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now let's predict sentiment.\n",
        "\n",
        "#### First we use the TestCase 1- Review level: Easy.\n",
        "\n"
      ],
      "metadata": {
        "id": "mcpbpG6sZOX0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predict_finetuned(\"While I hoped I would be wrong, I anticipated not being a fan of Wicked For Good…unfortunately, despite Ariana Grande and Cynthia Erivo singing their hearts out, it was a bit of a mess.\")\n"
      ],
      "metadata": {
        "id": "zMv_HkgdNp59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_pretrained(\"While I hoped I would be wrong, I anticipated not being a fan of Wicked For Good…unfortunately, despite Ariana Grande and Cynthia Erivo singing their hearts out, it was a bit of a mess.\")\n"
      ],
      "metadata": {
        "id": "QNLUev5RVBnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_svm(\"While I hoped I would be wrong, I anticipated not being a fan of Wicked For Good…unfortunately, despite Ariana Grande and Cynthia Erivo singing their hearts out, it was a bit of a mess.\")\n"
      ],
      "metadata": {
        "id": "uIz_iXfhVEdH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Review/Prompt: \"While I hoped I would be wrong, I anticipated not being a fan of Wicked For Good…unfortunately, despite Ariana Grande and Cynthia Erivo singing their hearts out, it was a bit of a mess.\"\n",
        "\n",
        "GPT response: negative\n",
        "\n",
        "Simulated Inference time: 8 ms"
      ],
      "metadata": {
        "id": "Z7kDTdcCchhW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Lets move to TestCase 2- Review complexity level: Medium\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6a7T2b_NZTl-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predict_finetuned(\"There's no way to make this documentary sound like the upbeat, rousing and often downright hilarious romp it is, but here goes: At the urging of comedian Tig Notaro, poet and spoken-word star Andrea Gibson and life partner and fellow poet Megan Falley invited filmmaker Ryan White and his crew into their home in 2021. It was mid-pandemic, and the crew was allowed full access to the couple's every thought and action as they dealt with turtledove love, mailbox madness, and, here's the part where you say, no, this does not sound like a good time ,Gibson's Stage 4 ovarian cancer journey. At the Middleburg Film Festival screening I attended in October, three months after Gibson's death, the director spoke beforehand, giving the audience permission to laugh, which it definitely did. It also sniffled a bit, but less than you might expect, because Gibson's vibrant, assertively affirmative outlook doesn't really brook tears, and the filmmaker's warmth and humor, even in times of despair, gives the story a radiance that makes mundane moments feel precious, while allowing hopeful moments to raise goosebumps.\")"
      ],
      "metadata": {
        "id": "JSN-p54kZsgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_pretrained(\"There's no way to make this documentary sound like the upbeat, rousing and often downright hilarious romp it is, but here goes: At the urging of comedian Tig Notaro, poet and spoken-word star Andrea Gibson and life partner and fellow poet Megan Falley invited filmmaker Ryan White and his crew into their home in 2021. It was mid-pandemic, and the crew was allowed full access to the couple's every thought and action as they dealt with turtledove love, mailbox madness, and, here's the part where you say, no, this does not sound like a good time ,Gibson's Stage 4 ovarian cancer journey. At the Middleburg Film Festival screening I attended in October, three months after Gibson's death, the director spoke beforehand, giving the audience permission to laugh, which it definitely did. It also sniffled a bit, but less than you might expect, because Gibson's vibrant, assertively affirmative outlook doesn't really brook tears, and the filmmaker's warmth and humor, even in times of despair, gives the story a radiance that makes mundane moments feel precious, while allowing hopeful moments to raise goosebumps.\")"
      ],
      "metadata": {
        "id": "3XDum9mYaDes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_svm(\"There's no way to make this documentary sound like the upbeat, rousing and often downright hilarious romp it is, but here goes: At the urging of comedian Tig Notaro, poet and spoken-word star Andrea Gibson and life partner and fellow poet Megan Falley invited filmmaker Ryan White and his crew into their home in 2021. It was mid-pandemic, and the crew was allowed full access to the couple's every thought and action as they dealt with turtledove love, mailbox madness, and, here's the part where you say, no, this does not sound like a good time ,Gibson's Stage 4 ovarian cancer journey. At the Middleburg Film Festival screening I attended in October, three months after Gibson's death, the director spoke beforehand, giving the audience permission to laugh, which it definitely did. It also sniffled a bit, but less than you might expect, because Gibson's vibrant, assertively affirmative outlook doesn't really brook tears, and the filmmaker's warmth and humor, even in times of despair, gives the story a radiance that makes mundane moments feel precious, while allowing hopeful moments to raise goosebumps.\")"
      ],
      "metadata": {
        "id": "KhPEZiyPaHNP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Review/Prompt: \"There's no way to make this documentary sound like the upbeat, rousing and often downright hilarious romp it is, but here goes: At the urging of comedian Tig Notaro, poet and spoken-word star Andrea Gibson and life partner and fellow poet Megan Falley invited filmmaker Ryan White and his crew into their home in 2021. It was mid-pandemic, and the crew was allowed full access to the couple's every thought and action as they dealt with turtledove love, mailbox madness, and, here's the part where you say, no, this does not sound like a good time ,Gibson's Stage 4 ovarian cancer journey. At the Middleburg Film Festival screening I attended in October, three months after Gibson's death, the director spoke beforehand, giving the audience permission to laugh, which it definitely did. It also sniffled a bit, but less than you might expect, because Gibson's vibrant, assertively affirmative outlook doesn't really brook tears, and the filmmaker's warmth and humor, even in times of despair, gives the story a radiance that makes mundane moments feel precious, while allowing hopeful moments to raise goosebumps.\"\n",
        "\n",
        "GPT Response: Positive\n",
        "\n",
        "Simulated Inference time: 14 ms"
      ],
      "metadata": {
        "id": "_8KWpyGVcZUb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Finally TestCase 3- Review complexity level: High\n"
      ],
      "metadata": {
        "id": "KOCwgm1QZX5r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predict_finetuned(\"Cannot understand the 1 out of 10 harsh review. 1 means its the worst film you have ever seen which this is not. Presumably its either because they were  offended by the film being directed, written and acted by a woman who was married to Elon Musk and is giving it the silver spoon brush off or either offended at a Scottish movie with mostly English actors in it, who knows but the film is worth a watch with your partner as it is most definitely not just a womans movie as I quite enjoyed it and am a bit of an old romantic at heart myself.Its not going to win any oscars but it has some moments of comedy magic in that typical Glasgow way which only us Weegies can fully appreciate. Not a bad effort at all from Talulah Riley at all, its just a shame that people want to bash her because she is the ex of Elon Musk which is a shame as she certainly has acting talent that is for sure and for her first directorial debut it was not half bad\")"
      ],
      "metadata": {
        "id": "3NhEhhJZVWqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_pretrained(\"Cannot understand the 1 out of 10 harsh review. 1 means its the worst film you have ever seen which this is not. Presumably its either because they were  offended by the film being directed, written and acted by a woman who was married to Elon Musk and is giving it the silver spoon brush off or either offended at a Scottish movie with mostly English actors in it, who knows but the film is worth a watch with your partner as it is most definitely not just a womans movie as I quite enjoyed it and am a bit of an old romantic at heart myself.Its not going to win any oscars but it has some moments of comedy magic in that typical Glasgow way which only us Weegies can fully appreciate. Not a bad effort at all from Talulah Riley at all, its just a shame that people want to bash her because she is the ex of Elon Musk which is a shame as she certainly has acting talent that is for sure and for her first directorial debut it was not half bad\")"
      ],
      "metadata": {
        "id": "uijSC9HwZfxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_svm(\"Cannot understand the 1 out of 10 harsh review. Presumably its either because they were  offended by the film being directed, written and acted by a woman who was married to Elon Musk and is giving it the silver spoon brush off or either offended at a Scottish movie with mostly English actors in it, who knows but the film is worth a watch with your partner as it is most definitely not just a womans movie as I quite enjoyed it and am a bit of an old romantic at heart myself.Its not going to win any oscars but it has some moments of comedy magic in that typical Glasgow way which only us Weegies can fully appreciate. Not a bad effort at all from Talulah Riley at all, its just a shame that people want to bash her because she is the ex of Elon Musk which is a shame as she certainly has acting talent that is for sure and for her first directorial debut it was not half bad\")"
      ],
      "metadata": {
        "id": "zHb7vmh2ZoLn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Review prompt: \"Cannot understand the 1 out of 10 harsh review. 1 means its the worst film you have ever seen which this is not. Presumably its either because they were  offended by the film being directed, written and acted by a woman who was married to Elon Musk and is giving it the silver spoon brush off or either offended at a Scottish movie with mostly English actors in it, who knows but the film is worth a watch with your partner as it is most definitely not just a womans movie as I quite enjoyed it and am a bit of an old romantic at heart myself.\n",
        "\n",
        "Its not going to win any oscars but it has some moments of comedy magic in that typical Glasgow way which only us Weegies can fully appreciate. Not a bad effort at all from Talulah Riley at all, its just a shame that people want to bash her because she is the ex of Elon Musk which is a shame as she certainly has acting talent that is for sure and for her first directorial debut it was not half bad!\"\n",
        "\n",
        "GPT Response: Positive\n",
        "\n",
        "Simulated Inference time: 11ms"
      ],
      "metadata": {
        "id": "ma5fqyc-b6PW"
      }
    }
  ]
}